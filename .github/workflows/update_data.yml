name: Update Trump Data (X-Edition)
on:
  schedule:
    - cron: '*/30 * * * *' # Körs varje halvtimme
  workflow_dispatch:

jobs:
  update:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Fetch X Data via Nitter
        run: |
          cat <<EOF > analyzer.py
          import urllib.request
          import json
          import datetime
          import random
          from xml.dom import minidom
          import ssl

          # Ignorera SSL-fel
          ssl._create_default_https_context = ssl._create_unverified_context

          # Lista på Nitter-instanser (Speglar av X)
          # Vi testar dem i tur och ordning
          nitter_instances = [
              "https://nitter.net/realDonaldTrump/rss",
              "https://nitter.cz/realDonaldTrump/rss",
              "https://nitter.privacydev.net/realDonaldTrump/rss",
              "https://nitter.poast.org/realDonaldTrump/rss",
              "https://unofficialbird.com/realDonaldTrump/rss"
          ]

          post_count_last_hour = 0
          status = "simulated"
          fetched_success = False
          
          now = datetime.datetime.utcnow()
          print(f"Tid (UTC): {now}")

          # --- FÖRSÖK HÄMTA FRÅN X (VIA NITTER) ---
          for url in nitter_instances:
              if fetched_success: break
              
              try:
                  print(f"Testar instans: {url} ...")
                  # Vi måste se ut som en webbläsare
                  req = urllib.request.Request(
                      url, 
                      headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
                  )
                  # 10 sekunders timeout per server
                  response = urllib.request.urlopen(req, timeout=10)
                  rss_content = response.read()
                  
                  # Om vi fick svar, parsa XML
                  dom = minidom.parseString(rss_content)
                  items = dom.getElementsByTagName('item')
                  
                  real_count = 0
                  print(f"SUCCESS! Fick kontakt. Analyserar {len(items)} inlägg...")

                  for item in items:
                      try:
                          # Nitter använder standard pubDate
                          pub_date_str = item.getElementsByTagName('pubDate')[0].firstChild.data
                          # Format: "Mon, 09 Feb 2026 12:00:00 GMT"
                          # Vi tar bort tidszonen på slutet för enkelhetens skull
                          pub_date = datetime.datetime.strptime(pub_date_str[:25], "%a, %d %b %Y %H:%M:%S")
                          
                          diff = now - pub_date
                          diff_minutes = diff.total_seconds() / 60
                          
                          if diff_minutes < 60:
                              real_count += 1
                      except Exception as e:
                          continue

                  post_count_last_hour = real_count
                  status = "live"
                  fetched_success = True
                  print(f"Resultat: {real_count} inlägg senaste timmen.")

              except Exception as e:
                  print(f"Instans misslyckades: {e}")
                  continue

          # --- FALLBACK: SIMULERING ---
          if not fetched_success:
              print("Alla Nitter-instanser nere. Kör simulering.")
              status = "estimated"
              
              # Samma sannolikhet som förut
              roll = random.randint(0, 100)
              if roll < 65: post_count_last_hour = 0
              elif roll < 90: post_count_last_hour = random.randint(1, 2)
              else: post_count_last_hour = random.randint(3, 6)

          # Spara resultatet
          output = {
              "postCount": post_count_last_hour,
              "status": status,
              "source": "X (Twitter)",
              "lastUpdated": datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
          }

          with open('data.json', 'w') as f:
              json.dump(output, f)
          EOF

          python analyzer.py

      - name: Commit and Push
        run: |
          git config user.name "github-actions"
          git config user.email "action@github.com"
          git add data.json
          git commit -m "X Data Update" || exit 0
          git push
